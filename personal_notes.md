# How do you define a Markov chain?

There are 3 items involved:
- State space, S [Finite/countable set of states]
- Initial distribution:
    Probability distribution of the Markov chain at time =0.
    $$\sum_{i \epsilon S} \pi_0(i)) = 1$$
- Probability transition rule
